{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Presentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc3pbNTO3dS8"
      },
      "source": [
        "import itertools\r\n",
        "import os\r\n",
        "import matplotlib.pylab as plt\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "# import math\r\n",
        "\r\n",
        "#checking framework versions\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "\r\n",
        "#for checking hardware\r\n",
        "import subprocess\r\n",
        "import pprint\r\n",
        "\r\n",
        "#mounting gDrive\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "#preprocessing\r\n",
        "\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "#callbacks\r\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\r\n",
        "\r\n",
        "#Confusion Matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\r\n",
        "\r\n",
        "\r\n",
        "#Andreea Stuff\r\n",
        "from tensorflow import keras\r\n",
        "from keras import layers\r\n",
        "from keras.layers.core import Dense, Activation, Flatten, Dropout\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.metrics import categorical_crossentropy, Accuracy\r\n",
        "from keras.models import Model\r\n",
        "from keras.models import save_model, load_model\r\n",
        "from keras.layers import Dense,GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX58VSPc4CAo"
      },
      "source": [
        "Version checking + Hardware checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riQXmy_f3kEc",
        "outputId": "f6449b1f-ed8a-4763-8940-e642b2110518"
      },
      "source": [
        "print(\"TF version:\", tf.__version__)\r\n",
        "print(\"Hub version:\", hub.__version__)\r\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\r\n",
        "tf.test.gpu_device_name()\r\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Hub version: 0.11.0\n",
            "GPU is NOT AVAILABLE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 14543054048938878537]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5LhfbpC4Nw4"
      },
      "source": [
        "sp = subprocess.Popen(['nvidia-smi', '-q'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n",
        "\r\n",
        "out_str = sp.communicate()\r\n",
        "out_list = str(out_str[0]).split('\\\\n')\r\n",
        "\r\n",
        "out_dict = {}\r\n",
        "\r\n",
        "for item in out_list:\r\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkhiSadh4gt5"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH_geeNN45Lt"
      },
      "source": [
        "## Select the TF2 SavedModel\r\n",
        "\r\n",
        "Here we select our  TF2 SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzqFnGWq4yaJ"
      },
      "source": [
        "module_selection = (\"inception_v3\", 299) #@param [\"(\\\"mobilenet_v2_100_224\\\", 224)\", \"(\\\"resnet_v2_50\\\", 224)\",\"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\r\n",
        "handle_base, pixels = module_selection\r\n",
        "\r\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)\r\n",
        "IMAGE_SIZE = (pixels, pixels)\r\n",
        "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\r\n",
        "\r\n",
        "# Names of the integer classes, i.e., 0 -> afraid, 1 -> angry, etc.\r\n",
        "class_names = ['afraid', 'angry', 'disgusted', 'happy', 'neutral', 'sad', 'surprised']\r\n",
        "\r\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0UHNIMA58M8"
      },
      "source": [
        "#Connecting to gDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmp1S8au55Nw"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Htt0wY6AgG"
      },
      "source": [
        "train_dir = \"/content/gdrive/MyDrive/EmotionRec/Affectnet\" \r\n",
        "test_val_dir = \"/content/gdrive/MyDrive/EmotionRec/KDEF\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9RBXHBq_Khx"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hh3E_6a-9X9"
      },
      "source": [
        "Dataset was modified in Greyscaled and cropped using Haarcascade. This was done in Comand Prompt because it shrinked the dataset by 1/3 and it proved a faster training. (30 min faster on the first batch). This also helped us in transfering the dataset due to cloud transfer constrains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLSj3aa-8Uo2"
      },
      "source": [
        "#Setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzVQesQK6CKa"
      },
      "source": [
        "preprocess_function = tf.keras.applications.inception_v3.preprocess_input #@param [\"tf.keras.applications.inception_v3.preprocess_input\",\"tf.keras.applications.resnet.preprocess_input\", \"tf.keras.applications.mobilenet.preprocess_input\"] {type:\"raw\", allow-input: true}\r\n",
        "train_batches = ImageDataGenerator(preprocessing_function= preprocess_function).flow_from_directory(directory=train_dir,target_size=(IMG_HEIGHT,IMG_WIDTH),classes=list(CLASS_NAMES),batch_size=BATCH_SIZE)\r\n",
        "valid_batches = ImageDataGenerator(preprocessing_function= preprocess_function).flow_from_directory(directory=test_val_dir,target_size=(IMG_HEIGHT,IMG_WIDTH),classes=list(CLASS_NAMES),batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqXzJWA761G0"
      },
      "source": [
        "train_img_batch,train_label_batch = next(train_batches) \r\n",
        "test_img_batch,test_label_batch=next(valid_batches)   #Iterating over the Generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmCNzS3u6kF-"
      },
      "source": [
        "train_label_batch,test_label_batch=train_label_batch.astype('uint32'),test_label_batch.astype('uint32') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfXeDiwr7C-n"
      },
      "source": [
        "train_ds=tf.data.Dataset.from_tensor_slices((train_img_batch,train_label_batch))\r\n",
        "test_ds=tf.data.Dataset.from_tensor_slices((test_img_batch,test_label_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmj4HzlJ7Iei"
      },
      "source": [
        "train_ds = train_ds.shuffle(1024).repeat().batch(BATCH_SIZE).cache()\r\n",
        "test_ds = test_ds.repeat().batch(BATCH_SIZE).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geYtsG0G7oad"
      },
      "source": [
        "# Defining the model\r\n",
        "\r\n",
        "All it takes is to put a linear classifier on top of the `feature_extractor_layer` with the Hub module.\r\n",
        "\r\n",
        "For speed, we start out with a non-trainable `feature_extractor_layer`, but you can also enable fine-tuning for greater accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r-7qUxu6_NS"
      },
      "source": [
        "First Phase Fine Tuning\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2tA-cYL6wPN"
      },
      "source": [
        "# # # FINE - TUNING \r\n",
        "# base_model.trainable = True\r\n",
        "\r\n",
        "# # Let's take a look to see how many layers are in the base model\r\n",
        "# print(\"Number of layers in the base model: \", len(base_model.layers))\r\n",
        "\r\n",
        "# # Fine-tune from this layer onwards\r\n",
        "# fine_tune_at = 100\r\n",
        "\r\n",
        "# # Freeze all the layers before the `fine_tune_at` layer\r\n",
        "# for layer in base_model.layers[:fine_tune_at]:\r\n",
        "#   layer.trainable =  False\r\n",
        "\r\n",
        "\r\n",
        "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),  # Optimizer\r\n",
        "#   # Loss function to minimize\r\n",
        "#   loss=keras.losses.CategoricalCrossentropy(),\r\n",
        "#   # List of metrics to monitor\r\n",
        "#   metrics=[keras.metrics.Accuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZfon0h77fxj"
      },
      "source": [
        "do_fine_tuning = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T_YqfW3M7LK"
      },
      "source": [
        "\r\n",
        "*   Regularisation - a process of introducing additional information in order to prevent overfitting. \r\n",
        "\r\n",
        "> L2 generally beats L1 in terms of accuracy and it is easier to adjust.\r\n",
        "\r\n",
        "> loss = l2 * reduce_sum(square(x))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XepD_lRP7q53"
      },
      "source": [
        "print(\"Building model with\", MODULE_HANDLE)\r\n",
        "\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    # Explicitly define the input shape so the model can be properly\r\n",
        "    # loaded by the TFLiteConverter\r\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\r\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\r\n",
        "    tf.keras.layers.Dropout(rate=0.2),\r\n",
        "    tf.keras.layers.Dense(len(class_names),activation='softmax',\r\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\r\n",
        "])\r\n",
        "model.build((None,)+IMAGE_SIZE+(3,)) #groups layers into an object with training and inference features.\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgX54oy-zxBA"
      },
      "source": [
        "Used for MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUbcbgA7zixO"
      },
      "source": [
        "# # functional method of adding layers\r\n",
        "\r\n",
        "# inputs = tf.keras.Input(shape=(img_width, img_height, 3))\r\n",
        "# x = data_augmentation(inputs)\r\n",
        "# x = resize_and_rescale(x)\r\n",
        "\r\n",
        "# x=base_model(x)\r\n",
        "\r\n",
        "# # x=base_model.output\r\n",
        "# x=GlobalAveragePooling2D()(x)\r\n",
        "# x=Flatten()(x) #connection between cov layers and dense layers\r\n",
        "\r\n",
        "# x=Dense(512,activation='relu')(x) \r\n",
        "# x=Dropout(0.5)(x)\r\n",
        "\r\n",
        "# x=Dense(512*2,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\r\n",
        "# x=Dropout(0.5)(x) #setting parameters to 0 which helps prevent overfitting\r\n",
        "\r\n",
        "# x=Dense(512*4,activation='relu')(x) #dense layer 2\r\n",
        "# x=Dropout(0.5)(x)\r\n",
        "# x=layers.BatchNormalization()(x)\r\n",
        "\r\n",
        "# preds=Dense(7,activation='softmax')(x) #final layer with softmax activation\r\n",
        "\r\n",
        "# model=Model(inputs=inputs,outputs=preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM3gItOj74BK"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hDGyizJQlJp"
      },
      "source": [
        "*   Stochastic gradient descent Optimizer\r\n",
        "\r\n",
        "  *   Optimizer helps us to minimize an error function(loss function)or to maximize the efficiency of production.\r\n",
        "  *   Momentum is like a ball rolling downhill. The ball will gain momentum as it rolls down the hill.\r\n",
        "*  Observation:\r\n",
        "  *   SGD without momentum , Updates takes longer vertical steps [slower learning]than horizontal step[faster learning].\r\n",
        "  *   SGD with momentum,Updates takes longer Horizontal steps[faster learning] then vertical step[slower learning].\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   SparseCategoricalCrossentropy is more efficient when you have a lot of categories.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwIyLPCI7znu"
      },
      "source": [
        "model.compile(\r\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \r\n",
        "  loss=keras.losses.SparseCategoricalCrossentropy(),\r\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f212kaQd0ObG"
      },
      "source": [
        "Used for the 1st Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mThcF37O0M7A"
      },
      "source": [
        "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),  # Optimizer close to 0.0075\r\n",
        "#   # Loss function to minimize\r\n",
        "#   loss=keras.losses.SparseCategoricalCrossentropy(), # vs CategoricalCrossentropy() because our data is aranges so that for each emotion we got a value, w\r\n",
        "#   # List of metrics to monitor\r\n",
        "#   metrics=[keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iudkt1v68pIy"
      },
      "source": [
        "#Checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuZ54DQVXBTh"
      },
      "source": [
        "ReduceLROnPlateau - Reduce learning rate when a metric has stopped improving.\r\n",
        "\r\n",
        "> Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKuHRISK0jtf"
      },
      "source": [
        "# CHECKPOINTS USED\r\n",
        "\r\n",
        "# early checkpoint. It stops when the validation dataset starts to degrade\r\n",
        "early = EarlyStopping(monitor='val_loss',patience=10,mode='auto')\r\n",
        "\r\n",
        "# create a model checkpoint\r\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "        filepath=\"/content/drive/MyDriveadded_dense_weights.hdf5\", # Path where to save the WEIGHTS\r\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\r\n",
        "        monitor=\"val_loss\",\r\n",
        "        mode='min', # i want the min value of the loss\r\n",
        "        verbose=1)\r\n",
        "\r\n",
        "# create a ReduceLROnPlateau callback\r\n",
        "\r\n",
        "reduce_lr = ReduceLROnPlateau(\r\n",
        "    monitor='val_loss', \r\n",
        "    factor=0.1,   # reduce the rate with 0.1\r\n",
        "    patience=1, \r\n",
        "    min_lr=0.0001,\r\n",
        "    verbose=1\r\n",
        ")\r\n",
        "callbacks_list = [early,model_checkpoint_callback,reduce_lr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrmmoD9F8mct"
      },
      "source": [
        "# #Automatic rename with epoch number and val accuracy:\r\n",
        "# from keras.callbacks import *\r\n",
        "# filepath=\"/content/gdrive/MyDrive/MyCNN/epochs:{epoch:03d}-val_accuracy:{val_accuracy:.3f}.hdf5\"\r\n",
        "# checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\r\n",
        "# callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtOskpY8vLZ"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2PjV5ciIyew"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Epochs-One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\r\n",
        "*   Batch Size - Total number of training examples present in a single batch.\r\n",
        "*   Batch - Defines the number of samples that will be propagated through the network.\r\n",
        "*   Iterations - The number of batches needed to complete one epoch.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EYNPjCq6z3z"
      },
      "source": [
        "STEPS_PER_EPOCH=np.ceil(len(train_batches.filenames)/BATCH_SIZE)\r\n",
        "VALIDATION_STEPS=np.ceil(len(valid_batches.filenames)/BATCH_SIZE)\r\n",
        "print(STEPS_PER_EPOCH , VALIDATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVieRNQJ8uud"
      },
      "source": [
        "# Train the classifier.\r\n",
        "hist = model.fit(\r\n",
        "    train_ds,\r\n",
        "    epochs=100, \r\n",
        "    steps_per_epoch=STEPS_PER_EPOCH ,\r\n",
        "    callbacks=callbacks_list,\r\n",
        "    validation_data=valid_ds,\r\n",
        "    validation_steps=VALIDATION_STEPS).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK-glZ5A823X"
      },
      "source": [
        "plt.figure()\r\n",
        "plt.ylabel(\"Loss (training and validation)\")\r\n",
        "plt.xlabel(\"Training Steps\")\r\n",
        "plt.ylim([0,2])\r\n",
        "plt.plot(hist[\"loss\"])\r\n",
        "plt.plot(hist[\"val_loss\"])\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\r\n",
        "plt.xlabel(\"Training Steps\")\r\n",
        "plt.ylim([0,1])\r\n",
        "plt.plot(hist[\"accuracy\"])\r\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttzWncV_2WbA"
      },
      "source": [
        "#Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Pg2WJW85J3"
      },
      "source": [
        "# COMPUTE CONFUSION MATRIX NORMALIZED\r\n",
        "\r\n",
        "Y_pred = model.predict(valid_generator, valid_generator.samples // BATCH_SIZE+1)\r\n",
        "y_pred = np.argmax(Y_pred, axis=1)\r\n",
        "\r\n",
        "print('Confusion Matrix')\r\n",
        "cm = confusion_matrix(valid_generator.classes, y_pred)\r\n",
        "\r\n",
        "# normalize the data\r\n",
        "cm = cm / cm.astype(np.float).sum(axis=1)[:, np.newaxis] # recall  (tp)/(tp+fn) \r\n",
        "\r\n",
        "print(cm)\r\n",
        "print('Classification Report')\r\n",
        "target_names = list(train_generator.class_indices.keys())\r\n",
        "print(classification_report(valid_generator.classes, y_pred, target_names=target_names))\r\n",
        "\r\n",
        "\r\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\r\n",
        "                              display_labels=valid_generator.classes)\r\n",
        "\r\n",
        "# make the plot bigger\r\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\r\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33ODC5bABUOu"
      },
      "source": [
        "Finally, the trained model can be saved for deployment to TF Serving or TF Lite (on mobile) as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRsrRJm8BT2i"
      },
      "source": [
        "model_name = \"Inception-v3.h5\"#@param[\"Inception-v3.h5\",\"ResNetV2.h5\",\"MobileNet.h5\"]\r\n",
        "model.save(model_name)\r\n",
        "model.save(\"/content/gdrive/MyDrive/MyCNN/\"+model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqwWZNRR7DnF"
      },
      "source": [
        "#Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s39teztE7KWb"
      },
      "source": [
        "# datagen_kwargs = dict(rescale=1./255, validation_split=.20)\r\n",
        "# dataflow_kwargs = dict(color_mode=\"rgb\",target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\r\n",
        "#                    interpolation=\"bilinear\")\r\n",
        "\r\n",
        "# valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n",
        "#     **datagen_kwargs)\r\n",
        "# valid_generator = valid_datagen.flow_from_directory(\r\n",
        "#     test_val_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs) #test_val_dir\r\n",
        "\r\n",
        "# do_data_augmentation = True #@param {type:\"boolean\"}\r\n",
        "# if do_data_augmentation:\r\n",
        "#   train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n",
        "#       rotation_range=40,\r\n",
        "#       horizontal_flip=True,\r\n",
        "#       width_shift_range=0.2, \r\n",
        "#       height_shift_range=0.2,\r\n",
        "#       shear_range=0.2, zoom_range=0.2,\r\n",
        "#       **datagen_kwargs)\r\n",
        "# else:\r\n",
        "#   train_datagen = valid_datagen\r\n",
        "# train_generator = train_datagen.flow_from_directory(\r\n",
        "#     train_dir, subset=\"training\", shuffle=True, **dataflow_kwargs) #train_dir\r\n",
        "\r\n",
        "\r\n",
        "# number_of_valid_examples = len(valid_generator.filenames)\r\n",
        "# number_of_generator_calls = math.ceil(number_of_valid_examples / (1.0 * BATCH_SIZE)) \r\n",
        "# # 1.0 above is to skip integer division\r\n",
        "\r\n",
        "# valid_labels = []\r\n",
        "\r\n",
        "# for i in range(0,int(number_of_generator_calls)):\r\n",
        "#     valid_labels.extend(np.array(valid_generator[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}